# DA6401: Introduction to Deep Learning

This repository contains the assignments, code, notebooks, and supporting files for the course **DA6401: Introduction to Deep Learning** (Jan–May 2026, Graduate Section – M.Tech/MS/PhD/Other students).

The goal of this repository is to provide **clear, well-structured implementations and analyses** of fundamental and advanced deep learning concepts, spanning theory, implementation from scratch, and modern deep learning frameworks.

---

## Course Learning Objectives

By completing the assignments in this repository, students will:

* **Foundations**: Understand the mathematical and conceptual foundations of deep learning, including neural network architectures and learning dynamics.
* **Optimization**: Gain hands-on experience with gradient-based optimization methods and regularization techniques.
* **Architectures**: Study and implement key deep learning models such as MLPs, CNNs, RNNs, LSTMs, and Transformers.
* **Implementation**: Build and train neural networks using NumPy and modern frameworks such as PyTorch.
* **Evaluation & Analysis**: Analyze model performance, compare optimization strategies, and reason about architectural and design choices.

---

## Course Content

Assignments and implementations are aligned with the official course syllabus, including:

* Introduction and History of Deep Learning
* Neural Network Fundamentals: Perceptrons, MLPs, Representation Power
* Feedforward Neural Networks and Automatic Differentiation
* Optimization Techniques: GD, Momentum, NAG, AdaGrad, RMSProp, Adam
* Regularization Techniques: Bias–Variance Tradeoff, L2, Dropout, Early Stopping, Data Augmentation
* Advanced Training Methods: Batch Normalization, Improved Activations
* Convolutional Neural Networks (CNNs): LeNet, AlexNet, VGG, ResNet
* Word Embeddings and Vector Representations
* Recurrent Neural Networks (RNNs), LSTMs, GRUs
* Encoder–Decoder Models and Attention Mechanisms
* Transformers and Self-Attention

---

## Assignments Overview

### Assignment 1: Neural Networks from Scratch (NumPy)

* Implement a simple Multilayer Perceptron (MLP) from scratch
* Manually derive and code forward propagation and backpropagation
* Apply the model to classify handwritten digits from the MNIST dataset

---

### Assignment 2: CNNs & Parametric Optimization (PyTorch)

* Train Convolutional Neural Networks on CIFAR-10
* Compare optimization methods (SGD vs Adam)
* Analyze the effect of different regularization techniques

---

### Assignment 3: Sequence Models (RNN / LSTM)

* Build a Sequence-to-Sequence (Encoder–Decoder) model
* Apply the model to basic text generation or translation tasks
* Study temporal dependencies and training challenges

---

### Assignment 4: Generative Models (GAN / ViT)

* Implement a Deep Convolutional GAN (DCGAN) for image generation
  **or** Implement and experiment with a Vision Transformer (ViT)
* Analyze generative performance and architectural trade-offs

---

## References

### Primary Textbooks

* Charu C. Aggarwal, **Neural Networks and Deep Learning**, Springer, 2019
* Aston Zhang et al., **Dive into Deep Learning**

### Reference Books

* Ian Goodfellow, Yoshua Bengio, Aaron Courville, **Deep Learning**, MIT Press, 2016
* Simon J. D. Prince, **Understanding Deep Learning**, MIT Press, 2023

---

## Prerequisites

* No formal prerequisites
* Familiarity with linear algebra, probability, and Python programming is helpful

---

## Acknowledgements

This repository was created as part of the **DA6401: Introduction to Deep Learning** course at **IIT Madras**.
